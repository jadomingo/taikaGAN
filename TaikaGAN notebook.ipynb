{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TaikaGAN Project Documentation\n",
    "\n",
    "Members:\n",
    "1. Bautista, Michael\n",
    "2. Domingo, Jaime\n",
    "3. Tan, Allen\n",
    "\n",
    "This serves as documentation for TaikaGAN, a Two-Domain Image-to-image Translation for Anime-styled Illustrations, which uses CycleGAN as its basis.\n",
    "\n",
    "This project is based on the generator and discriminator models used in https://yanjia.li/gender-swap-and-cyclegan-in-tensorflow-2-0/. Since we didn't understand Tensorflow enough to have the capability to create Keras equivalents based on the Tensorflow code used in that implementation alone, we decided to use code from https://machinelearningmastery.com/cyclegan-tutorial-with-keras/ to train the model, and to allow for setting up the preprocessing necessary for the images.\n",
    "\n",
    "The dataset was scraped from http://getchu.com (warning:NSFW), and the scraper and face detection tool used was adapted from https://github.com/Mckinsey666/Anime-Face-Dataset. This dataset was sorted manually over several days.\n",
    "The dataset used had a total of 1254 male and 1254 female faces. These faces were downscaled to 128x128, and then upscaled to 256x256 in order to fit the input size of the CycleGAN model we used.\n",
    "\n",
    "The following references were also used to make the project work:\n",
    "- https://stackoverflow.com/questions/52826134/keras-model-subclassing-examples (for facilitating saving weights with the generator in particular, which involves ResNet blocks that are a subclass of the Keras Model class)\n",
    "- https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers for helping with making the scraper work (not directly related to making the model work, but necessary for acquiring the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an overview of the Cycle part of the CycleGAN. The two blocks on top are the generators, the block on the bottom-left is the discriminator.\n",
    "<img src = \"overview.png\">\n",
    "\n",
    "The generator was the identical to the reference model, but with 6 instead of 9 ResNet blocks used.\n",
    "<img src = \"generator.png\">\n",
    "\n",
    "We also decided to use the same discriminator in the interest of saving time.\n",
    "<img src = \"discriminator.png\">\n",
    "\n",
    "The discriminator ends in a PatchGAN, just like the original CycleGAN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SUPPORT CODE FOR THE REST OF THE DOCUMENTATION ###\n",
    "# fix for memory issues\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from models import make_generator_model\n",
    "from preprocess import load_saved_dataset\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy.random import randint\n",
    "from numpy import vstack\n",
    "\n",
    "# select a random sample of images from the dataset\n",
    "def select_sample(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = dataset[ix]\n",
    "\treturn X\n",
    " \n",
    "# plot the image, the translation, and the reconstruction\n",
    "def show_plot(imagesX, imagesY1, imagesY2):\n",
    "\timages = vstack((imagesX, imagesY1, imagesY2))\n",
    "\ttitles = ['Real', 'Generated', 'Reconstructed']\n",
    "\t# scale from [-1,1] to [0,1]\n",
    "\timages = (images + 1) / 2.0\n",
    "\t# plot images row by row\n",
    "\tfor i in range(len(images)):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(1, len(images), 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tpyplot.imshow(images[i])\n",
    "\t\t# title\n",
    "\t\tpyplot.title(titles[i%3])\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATASET ###\n",
    "m, f = load_saved_dataset('proj_dataset.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD GENERATOR MODELS ###\n",
    "gen_1_model_f2m = make_generator_model(6)\n",
    "#gen_1_model_f2m.load_weights('g_model_BtoA_001254.h5')\n",
    "# gen_1_model_m2f = make_generator_model(6)\n",
    "# gen_1_model_m2f.load_weights('g_model_AtoB_001254.h5')\n",
    "# load other epochs of generators here\n",
    "gen_last_model_f2m = make_generator_model(6)\n",
    "gen_last_model_f2m.load_weights('g_model_BtoA_025080.h5')\n",
    "# gen_last_model_m2f = make_generator_model(6)\n",
    "# gen_last_model_f2m.load_weights('g_model_AtoB_003762.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHOW EXAMPLE(S) ###\n",
    "print('Epoch 1:')\n",
    "m_real = select_sample(f, 1)\n",
    "f_real = select_sample(m, 1)\n",
    "\n",
    "f2m = gen_1_model_f2m.predict(f_real)\n",
    "#f2m2f = gen_1_model_m2f.predict(f2m)\n",
    "#m2f = gen_1_model_m2f.predict(m_real)\n",
    "#m2f2m = gen_1_model_f2m.predict(m2f)\n",
    "\n",
    "#print('Male pictures:')\n",
    "#show_plot(m_real, f2m, f2m2f)\n",
    "print('Female pictures:')\n",
    "show_plot(m_real, f2m, f2m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHOW EXAMPLE(S) ###\n",
    "print('Epoch 20:')\n",
    "m_real = select_sample(f, 1)\n",
    "f_real = select_sample(m, 1)\n",
    "\n",
    "f2m = gen_last_model_f2m.predict(f_real)\n",
    "#f2m2f = gen_1_model_m2f.predict(f2m)\n",
    "#m2f = gen_1_model_m2f.predict(m_real)\n",
    "#m2f2m = gen_1_model_f2m.predict(m2f)\n",
    "\n",
    "#print('Male pictures:')\n",
    "#show_plot(m_real, f2m, f2m2f)\n",
    "print('Female pictures:')\n",
    "show_plot(m_real, f2m, f2m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
